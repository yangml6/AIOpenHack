{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk.device import try_set_default_device, gpu\n",
    "\n",
    "from __future__ import print_function\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Some of the flowers data is stored as .mat files\n",
    "from scipy.io import loadmat\n",
    "from shutil import copyfile\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "# Loat the right urlretrieve based on python version\n",
    "try: \n",
    "    from urllib.request import urlretrieve \n",
    "except ImportError: \n",
    "    from urllib import urlretrieve\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Useful for being able to dump images into the Notebook\n",
    "import IPython.display as D\n",
    "\n",
    "# Import CNTK and helpers\n",
    "import cntk as C\n",
    "\n",
    "\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "isFast = True\n",
    "\n",
    "# By default, we store data in the Examples/Image directory under CNTK\n",
    "# If you're running this _outside_ of CNTK, consider changing this\n",
    "data_root = os.path.join('..', 'Examples', 'Image')\n",
    "\n",
    "datasets_path = os.path.join(data_root, 'DataSets')\n",
    "output_path = os.path.join('.', 'temp', 'Output')\n",
    "\n",
    "def ensure_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def write_to_file(file_path, img_paths, img_labels):\n",
    "    with open(file_path, 'w+') as f:\n",
    "        for i in range(0, len(img_paths)):\n",
    "            f.write('%s\\t%s\\n' % (os.path.abspath(img_paths[i]), img_labels[i]))\n",
    "\n",
    "def download_unless_exists(url, filename, max_retries=3):\n",
    "    '''Download the file unless it already exists, with retry. Throws if all retries fail.'''\n",
    "    if os.path.exists(filename):\n",
    "        print('Reusing locally cached: ', filename)\n",
    "    else:\n",
    "        print('Starting download of {} to {}'.format(url, filename))\n",
    "        retry_cnt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                urlretrieve(url, filename)\n",
    "                print('Download completed.')\n",
    "                return\n",
    "            except:\n",
    "                retry_cnt += 1\n",
    "                if retry_cnt == max_retries:\n",
    "                    print('Exceeded maximum retry count, aborting.')\n",
    "                    raise\n",
    "                print('Failed to download, retrying.')\n",
    "                time.sleep(np.random.randint(1,10))\n",
    "\n",
    "def download_model(model_root = os.path.join(data_root, 'PretrainedModels')):\n",
    "    ensure_exists(model_root)\n",
    "    resnet18_model_uri = 'https://www.cntk.ai/Models/ResNet/ResNet_18.model'\n",
    "    resnet18_model_local = os.path.join(model_root, 'ResNet_18.model')\n",
    "    download_unless_exists(resnet18_model_uri, resnet18_model_local)\n",
    "    return resnet18_model_local\n",
    "\n",
    "print('Downloading pre-trained model. Note: this might take a while...')\n",
    "base_model_file = download_model()\n",
    "print('Downloading pre-trained model complete!')\n",
    "\n",
    "# define base model location and characteristics\n",
    "base_model = {\n",
    "    'model_file': base_model_file,\n",
    "    'feature_node_name': 'features',\n",
    "    'last_hidden_node_name': 'z.x',\n",
    "    # Channel Depth x Height x Width\n",
    "    'image_dims': (3, 224, 224)\n",
    "}\n",
    "\n",
    "# Print out all layers in the model\n",
    "print('Loading {} and printing all layers:'.format(base_model['model_file']))\n",
    "node_outputs = C.logging.get_node_outputs(C.load_model(base_model['model_file']))\n",
    "for l in node_outputs: print(\"  {0} {1}\".format(l.name, l.shape))\n",
    "\n",
    "def plot_images(images, subplot_shape):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axes = plt.subplots(*subplot_shape)\n",
    "    for image, ax in zip(images, axes.flatten()):\n",
    "        ax.imshow(image.reshape(28, 28), vmin = 0, vmax = 1.0, cmap = 'gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "flowers_image_dir = os.path.join(flowers_data['data_folder'], 'extracted', 'jpg')\n",
    "\n",
    "for image in ['08093', '08084', '08081', '08058']:\n",
    "    D.display(D.Image(os.path.join(flowers_image_dir, 'image_{}.jpg'.format(image)), width=100, height=100))\n",
    "\n",
    "\n",
    "import cntk.io.transforms as xforms\n",
    "ensure_exists(output_path)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Creates a minibatch source for training or testing\n",
    "def create_mb_source(map_file, image_dims, num_classes, randomize=True):\n",
    "    transforms = [xforms.scale(width=image_dims[2], height=image_dims[1], channels=image_dims[0], interpolations='linear')]\n",
    "    return C.io.MinibatchSource(C.io.ImageDeserializer(map_file, C.io.StreamDefs(\n",
    "            features=C.io.StreamDef(field='image', transforms=transforms),\n",
    "            labels=C.io.StreamDef(field='label', shape=num_classes))),\n",
    "            randomize=randomize)\n",
    "\n",
    "# Creates the network model for transfer learning\n",
    "def create_model(model_details, num_classes, input_features, new_prediction_node_name='prediction', freeze=False):\n",
    "    # Load the pretrained classification net and find nodes\n",
    "    base_model = C.load_model(model_details['model_file'])\n",
    "    feature_node = C.logging.find_by_name(base_model, model_details['feature_node_name'])\n",
    "    last_node = C.logging.find_by_name(base_model, model_details['last_hidden_node_name'])\n",
    "\n",
    "    # Clone the desired layers with fixed weights\n",
    "    cloned_layers = C.combine([last_node.owner]).clone(\n",
    "        C.CloneMethod.freeze if freeze else C.CloneMethod.clone,\n",
    "        {feature_node: C.placeholder(name='features')})\n",
    "\n",
    "    # Add new dense layer for class prediction\n",
    "    feat_norm = input_features - C.Constant(114)\n",
    "    cloned_out = cloned_layers(feat_norm)\n",
    "    z = C.layers.Dense(num_classes, activation=None, name=new_prediction_node_name) (cloned_out)\n",
    "\n",
    "    return z\n",
    "\n",
    "# Trains a transfer learning model\n",
    "def train_model(model_details, num_classes, train_map_file,\n",
    "                learning_params, max_images=-1):\n",
    "    num_epochs = learning_params['max_epochs']\n",
    "    epoch_size = sum(1 for line in open(train_map_file))\n",
    "    if max_images > 0:\n",
    "        epoch_size = min(epoch_size, max_images)\n",
    "    minibatch_size = learning_params['mb_size']\n",
    "\n",
    "    # Create the minibatch source and input variables\n",
    "    minibatch_source = create_mb_source(train_map_file, model_details['image_dims'], num_classes)\n",
    "    image_input = C.input_variable(model_details['image_dims'])\n",
    "    label_input = C.input_variable(num_classes)\n",
    "\n",
    "    # Define mapping from reader streams to network inputs\n",
    "    input_map = {\n",
    "        image_input: minibatch_source['features'],\n",
    "        label_input: minibatch_source['labels']\n",
    "    }\n",
    "\n",
    "    # Instantiate the transfer learning model and loss function\n",
    "    tl_model = create_model(model_details, num_classes, image_input, freeze=learning_params['freeze_weights'])\n",
    "    ce = C.cross_entropy_with_softmax(tl_model, label_input)\n",
    "    pe = C.classification_error(tl_model, label_input)\n",
    "\n",
    "    # Instantiate the trainer object\n",
    "    lr_schedule = C.learning_rate_schedule(learning_params['lr_per_mb'], unit=C.UnitType.minibatch)\n",
    "    mm_schedule = C.momentum_schedule(learning_params['momentum_per_mb'])\n",
    "    learner = C.momentum_sgd(tl_model.parameters, lr_schedule, mm_schedule, \n",
    "                           l2_regularization_weight=learning_params['l2_reg_weight'])\n",
    "    trainer = C.Trainer(tl_model, (ce, pe), learner)\n",
    "\n",
    "    # Get minibatches of images and perform model training\n",
    "    print(\"Training transfer learning model for {0} epochs (epoch_size = {1}).\".format(num_epochs, epoch_size))\n",
    "    C.logging.log_number_of_parameters(tl_model)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    # loop over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        sample_count = 0\n",
    "        # loop over minibatches in the epoch\n",
    "        while sample_count < epoch_size: \n",
    "            data = minibatch_source.next_minibatch(min(minibatch_size, epoch_size - sample_count), input_map=input_map)\n",
    "            # update model with it\n",
    "            trainer.train_minibatch(data)\n",
    "            # count samples processed so far\n",
    "            sample_count += trainer.previous_minibatch_sample_count\n",
    "            # log progress\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "            if sample_count % (100 * minibatch_size) == 0:\n",
    "                print (\"Processed {0} samples\".format(sample_count))\n",
    "\n",
    "        progress_printer.epoch_summary(with_metric=True)\n",
    "\n",
    "    return tl_model\n",
    "\n",
    "\n",
    "# Evaluates a single image using the re-trained model\n",
    "def eval_single_image(loaded_model, image_path, image_dims):\n",
    "    # load and format image (resize, RGB -> BGR, CHW -> HWC)\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "        if image_path.endswith(\"png\"):\n",
    "            temp = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "            temp.paste(img, img)\n",
    "            img = temp\n",
    "        resized = img.resize((image_dims[2], image_dims[1]), Image.ANTIALIAS)\n",
    "        bgr_image = np.asarray(resized, dtype=np.float32)[..., [2, 1, 0]]\n",
    "        hwc_format = np.ascontiguousarray(np.rollaxis(bgr_image, 2))\n",
    "\n",
    "        # compute model output\n",
    "        arguments = {loaded_model.arguments[0]: [hwc_format]}\n",
    "        output = loaded_model.eval(arguments)\n",
    "\n",
    "        # return softmax probabilities\n",
    "        sm = C.softmax(output[0])\n",
    "        return sm.eval()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not open (skipping file): \", image_path)\n",
    "        return ['None']\n",
    "\n",
    "\n",
    "\n",
    "# Evaluates an image set using the provided model\n",
    "def eval_test_images(loaded_model, output_file, test_map_file, image_dims, max_images=-1, column_offset=0):\n",
    "    num_images = sum(1 for line in open(test_map_file))\n",
    "    if max_images > 0:\n",
    "        num_images = min(num_images, max_images)\n",
    "    if isFast:\n",
    "        #We will run through fewer images for test run\n",
    "        num_images = min(num_images, 300)\n",
    "\n",
    "    print(\"Evaluating model output node '{0}' for {1} images.\".format('prediction', num_images))\n",
    "\n",
    "    pred_count = 0\n",
    "    correct_count = 0\n",
    "    np.seterr(over='raise')\n",
    "    with open(output_file, 'wb') as results_file:\n",
    "        with open(test_map_file, \"r\") as input_file:\n",
    "            for line in input_file:\n",
    "                tokens = line.rstrip().split('\\t')\n",
    "                img_file = tokens[0 + column_offset]\n",
    "                probs = eval_single_image(loaded_model, img_file, image_dims)\n",
    "\n",
    "                if probs[0]=='None':\n",
    "                    print(\"Eval not possible: \", img_file)\n",
    "                    continue\n",
    "\n",
    "                pred_count += 1\n",
    "                true_label = int(tokens[1 + column_offset])\n",
    "                predicted_label = np.argmax(probs)\n",
    "                if predicted_label == true_label:\n",
    "                    correct_count += 1\n",
    "\n",
    "                #np.savetxt(results_file, probs[np.newaxis], fmt=\"%.3f\")\n",
    "                if pred_count % 100 == 0:\n",
    "                    print(\"Processed {0} samples ({1:.2%} correct)\".format(pred_count, (float(correct_count) / pred_count)))\n",
    "                if pred_count >= num_images:\n",
    "                    break\n",
    "    print (\"{0} of {1} prediction were correct\".format(correct_count, pred_count))\n",
    "    return correct_count, pred_count, (float(correct_count) / pred_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
